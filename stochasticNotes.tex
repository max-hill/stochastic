
\documentclass{article}
\usepackage[utf8]{inputenc}

% Math Packages
\usepackage{float}
\usepackage{amsmath, mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{breqn}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{forest}
\usepackage{tikz-qtree}
\graphicspath{ {./images/} }
\usepackage{hyperref}
\usepackage[shortlabels]{enumitem}
\usetikzlibrary{arrows,matrix,positioning}
\usepackage[ruled,vlined]{algorithm2e}
% References
\usepackage[capitalize]{cleveref}
 
% This code makes widecheck for Fourier transform. Usage: \widecheck{blah blach}
\makeatletter
\DeclareRobustCommand\widecheck[1]{{\mathpalette\@widecheck{#1}}}
\def\@widecheck#1#2{%
  \setbox\z@\hbox{\m@th$#1#2$}%
  \setbox\tw@\hbox{\m@th$#1%
    \widehat{%
      \vrule\@width\z@\@height\ht\z@
      \vrule\@height\z@\@width\wd\z@}$}%
  \dp\tw@-\ht\z@
  \@tempdima\ht\z@ \advance\@tempdima2\ht\tw@ \divide\@tempdima\thr@@
  \setbox\tw@\hbox{%
    \raise\@tempdima\hbox{\scalebox{1}[-1]{\lower\@tempdima\box
        \tw@}}}%
  {\ooalign{\box\tw@ \cr \box\z@}}}
\makeatother

% Colorful Notes
\usepackage{color}
\definecolor{Red}{rgb}{1,0,0}
\definecolor{Blue}{rgb}{0,0,1}
\definecolor{Purple}{rgb}{.5,0,.5}
\def\red{\color{Red}}
\def\blue{\color{Blue}}
\def\gray{\color{gray}}
\def\purple{\color{Purple}}

\newcommand{\rnote}[1]{{\red [#1] }} % \rnote{foo} then 'foo' is red
\newcommand{\bnote}[1]{{\blue #1}} % \bnote{foo} then 'foo' is blue
\newcommand{\gnote}[1]{{\gray [#1]}} % gray note  -- for comments
\newcommand{\pnote}[1]{{\purple [#1]}} % green note  -- for comments

% Math Environments
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example} 
\newtheorem{question}{Question}
\newtheorem{claim}{Claim} 
\newtheorem{conjecture}{Conjecture} 

% Custom Math Commands
\newcommand{\vt}{\vskip 5mm} % vertical space
\newcommand{\fl}{\noindent\textbf} % first line
\newcommand{\Fl}{\vt\noindent\textbf} % first line with space above
\def\R{\mathbb{R}} % Real numbers
\def\Z{\mathbb{Z}} % Integers
\def\E{\mathbb{E}} % Expectation
\def\P{\mathbb{P}} % Probability
\def\Q{\mathbb{Q}} % Q probability
\newcommand{\indep}{\perp \!\!\! \perp}  %independence symbol
\newcommand{\ifelse}[4]{\left\{ \begin{array}{l@{\quad:\quad}l}#1&#3\\#2&#4\end{array}\right.}
\newcommand{\curly}[2]{\left\{ \begin{array}{l@{\quad}l}#1\\#2\end{array}\right.}
\newcommand{\di}{\left. \mathrm{Di} \right|}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[shortlabels]{enumitem}
\newtheorem{problem}{Problem}

% Concise Vector Macro: write bracket vectors of arbitrary length
% Usage: \columnvector{x,y,1} or \columnvector{2,x^2}
\usepackage{stackengine}
\newcommand\columnvector[1]{\setstackEOL{,}\bracketVectorstack{#1}}


\title{Lecture Notes for Mihaela Ifrim's Course on Dispersive PDE}
\date{Fall 2022}


\begin{document}
\title{Stochastic PDE}
\author{Max Hill}
\maketitle
\tableofcontents
\newpage
\section{2023-01-25: Lecture}

Some examples of PDEs are
\begin{itemize}
  \item Laplace equation $\Delta u = 0$ (``elliptic'')
  \item Heat Equation (heat conduction) $\partial_{t}u = \Delta u $
  \item Transport Equation $\partial_{t}u + \partial_{x}u = 0$ (first order
  linear)
  \item Shrodinger Equation $i \partial_{t}u + \Delta u = 0$
  \item Wave Propagation $\delta_{t}^{2}u = \Delta u$ (``hyperbolic'')
  \item Maxwell Equation  
  \item Burger's Equation $\partial_{t}u = u\partial_{x}u +\Delta u $
  \item Navier-Stokes
  $\delta_{t}u = \Delta u - u \cdot Du + \nabla p , \quad \mathrm{div}\ u =0$
  where $u = (u_{1},\ldots,u_{n})$. 
 \item KDV Equation  $u_{t}+6uu_{x}+\partial_{x}^{3}u=0 $
 \item Hamilton-Jacobi (Classical mechanics/optimization)
 $\partial_{t}u=\Delta u+F(Du) $
\end{itemize}
PDEs offer a way to model the real world. But in the real world, we might have
some randomness, such as random stirring, random energy, thermal fluctuation,
quantum fluctuation, etc. For example, we can consider the following
modification of the heat equation:
\begin{equation*}
  \partial_{t}u = \Delta u +\xi(t,x)
\end{equation*}
where $\xi$ is a white noise, which we will define precisely later, but here
just note it is Gaussian and is defined by the following covariance property:
\begin{equation}
  \label{eq:1}
  \E\left[\xi(t,x)\xi(t',x') \right] = \delta(t-t')\delta(x-x')
\end{equation}
where $\delta$ is the Dirac delta function. The delta function is a generalized
function, i.e. an element in the dual of the smooth functions. \cref{eq:1} says
that the noise at different times is independent. White noise is important
because we often are interested in the case where the noise is small scales. One
drawback is that white noise is a generalized function, and consequently the
solutions to stochastic PDEs are often generalized functions as well (sometimes
called ``weak solutions'') which means the solutions may not be defined
pointwise. This is okay with linear PDEs but introduces serious well-posedness
questions for nonlinear PDEs (e.g., what does a nonlinear term like
$u_{i}\partial_{x}u$ mean if $u$ is not defined pointwise?).

To summarize,
\begin{align*}
  &\text{one variable}\rightarrow \text{ODE}\rightarrow \text{SDE}\\
  &\text{multivariate}\rightarrow \text{PDE}\rightarrow \text{SPDE} 
\end{align*}

Main applications for SPDE are
\begin{itemize}
  \item Macroscopic limits of microscopic models (e.g. models from statistical
  physics). 
  \item Quantum Field Theory QFT
\end{itemize}

One example of the former application is the Ising model. This is about magnets.
Consider a magnet $N|S$ which has a north and south end, and thus generates an
electromagnetic field. If you heat up the north end, the field will decrease.
This continues until a critical temperature $T_{c}$ (the ``Curie temperature'')
is reached, at which point the field disappears completely. What is going on?
Inside the magnet, the particles are like little magnets. At low temperatures,
they are all pointing more or less the same way, like arrows. At high
temperatures, the directions of the little magnets are random, like arrows all
pointing in random directions, and they cancel each other out.

Consideration of the ``little magnets'' is the microscopic behavior. One thing
we can do is send $T \to T_{c}$ while zooming out at the same time in order to
characterize the macroscopic behavior. This gives the following equation:
\begin{equation*}
  \partial_{t}\Phi = \Delta \Phi - \Phi^{3}+ \Phi
\end{equation*}


Another example, of the latter application (QFT), is the following. It involves
a ``measure'' on the space of functions/fields
\begin{equation*}
  \left\{\Phi \text{ on } \R^d \right\}
\end{equation*}
such as
\begin{equation*}
  e^{-\int_{\R^d }|\Delta \Phi(x)|^{2}dx }
\end{equation*}
Gaussian Free Field (particles don't interact). Another example, in which
particles do interact, is the ``$\Phi^{4}$ equation'' which is given by 
\begin{equation*}
  e^{-\int_{\R^d }|\Delta \Phi(x)|^{2} +\Phi^{4}dx }
\end{equation*}


Next time we will introduce white noise and how to deal with linear equations.
\section{2023-02-01: Lecture Notes}
First, review the Fourier Transform:
\begin{equation*}
  u(x) = \int_{\R^d } \hat{u}(k)e^{i \left\langle k,x \right\rangle} dk
\end{equation*}
and on the torus $T^{d}$, we have
\begin{equation*}
  u(x) = \sum_{z\in \Z^{d}} u_{k}e^{i \left\langle k,x \right\rangle}
\end{equation*}
If $u$ is real valued, then $\hat{u}_{k}=u_{-k}$. Another useful fact is the
Parseval identity which says that
\begin{equation*}
  \int u^{2}(x)dx =  
  \int \hat{u}^{2}(k)dk
\end{equation*}
and the RHS is $\sum_{z\in \Z^{d}}u_{k}^{2}$ if $u$ is defined on the Torus.

Last time, we discussed the equation
\begin{equation*}
  \left\{ \begin{array}{l@{\quad}l} \partial_{t}u =\Delta u+\xi \\
    u|_{t=0}=u_{0}\end{array}\right.
\end{equation*}
and $\lambda=dtdx$, $\xi(dtdx) $ (something.) 

Shorthand notation:
\begin{equation*}
  e^{t\Delta }u = P_{t}u_{0}= \int_{\R^d } p(t,x-y)u_{0}(y)dy 
\end{equation*}

and we have a solution to the differential equation above given by the formula
\begin{align*}
  u(t,x)
  &= \int_{\R^d }p(t,x-y) u_{0}(y)dy + \int_{\R^d }\int_{0}^{t}p(t-s,x-y)  \xi(dsdy)\\
  & e^{t\Delta }u_{0} + \int_{\R^d }\int_{(-\infty,\infty)}p(t-s,x-y)\mathbf{1}_{\left[ s \geq 0\right] }\xi(dsdy)\\
  & e^{t\Delta }u_{0} + p*(1_{+}\xi)(t,x)
\end{align*}
where $*$ denotes the spacetime convolution. This formula makes sense only for
$d=1$, as the convolution term is well-defined in $L^{2}(\Omega)$ in that case.
But for $d \geq 2$, this may not be the case. We will show this.

\begin{align*}
  \norm{p*(1_{+}\xi)}_{L^{2}(\Omega)}^{2}
  &=\int_{\R^d }\int_{0}^{t}p(s,y)^{2}dsdy  
\end{align*}
The issue is that $p(s,y)$ may have an integrable singularity, and there's no
guarantee of integrability if we square that. But observe that
\begin{equation*}
  \hat{p}(s,k) = e^{-s|k|^{2}}
\end{equation*}
and we note that the RHSis the characteristic function of a gaussian. By
Parseval, we have
\begin{align*}
  \norm{p*(1_{+}\xi)}_{L^{2}(\Omega)}^{2}
  &= \int_{0}^{t}\int_{\R^d }e^{-2s|k|^{2}}  dkds\\
  &= \int_{0}^{t}\int_{\R^d }\frac{1-e^{-2s|k|^{2}}}{2|k|^{2}}  dkds
\end{align*}
Is this integrable? Well if $k$ is near zero, there is no problem, since there
is no singularity at the origin. When $k\to\infty$, however, you are essentiall
integrating $\frac{1}{|k|^{2}}$ over $\R^d$.  This is finite if $d=1$ and
infinite if $d \geq 2$. This is called an 'ultraviolet problem' (think of $k$ as
spectrum of light, and 'ultraviolet' means $k$ is large).


\begin{theorem}[Kolmolgorov]
  \label{thm:kolmolgorov}
  For any complete metric space $(S,\rho)$ with $D\subseteq \R^n$. Suppose that
  $\left\{X_{s}:s\in D\right\}$ is an $S$-valued process on some probability
  space $\left( \Xi,\mathcal{F},P \right)$ such that
  \begin{equation*}
    \E\left[\rho(X_{s},X_{t})^{\beta} \right] \leq C \left| s-t \right|^{n+\alpha}
  \end{equation*}
  then $X$ has a continuous version $Y$. Moreover, $Y$ is $\gamma$-Holder
  condtinuous whenever $0<\gamma<\alpha/\beta$. 
\end{theorem}
\begin{example}[Application of Kolmologorv Theorem to Brownian Motion]
  For $n=1$ dimensional, $\R$-valued Brownian motion, we have $\alpha=1$ and
  $\beta=4$.
\end{example}
Last time, we looked at the increments $\xi(t,x)-\xi(s,y)$  of white noise and showed that
\begin{equation*}
  \E\left[\left| \xi(t,x)-\xi(s,y) \right|
    ^{p}\right] \leq C \left( \left| s-t \right|^{p/4}+\left| x-y \right|^{1/2}\right).
\end{equation*}


Now let's do some more stuff with white noise. We will consider the
$d$-dimensional torus $T^{d}$, because things are always easier with finite
domain. The Sobolov space $H^{s}(T^{d})$, $s \geq 0$,  is the functions
satsfying
\begin{equation*}
  \norm{h}_{H^{s}}^{2} := \sum_{k\in \Z^{d}}\left( 1+|k|^{2} \right)^{s}|u_{k}|^{2}<\infty  
\end{equation*}
Observe that the term $\left( 1+|k|^{2} \right)^{s}$ makes the summands harder
to integrate, and hence it follows that
\begin{equation*}
  H^{s}\subset L^{2}.
\end{equation*}
In particular, we define $H^{s}$ to be the closure of $L^{2}$ with respect to
the norm defined above. We also have
\begin{equation*}
  \cdots \subset H^{2}\subset H^{1} \subset L^{2} = H^{0} \subset H^{1} \subset H^{2}\subset \cdots
\end{equation*}
Recall
\begin{equation*}
  \partial_{t}u = \Delta u+\xi
\end{equation*}
and
\begin{equation*}
  dX = AX+dB, \quad B=(B_{1},\ldots,B_{n})
\end{equation*}
Process in $\infty$-dimensional space. Cylindrical BM is the name for
infinite-dimensional BM.
\begin{equation} \label{eq:2}
  B(t) = \sum_{k\in \Z^{d}}\beta_{k}(t)e_{k} 
\end{equation}
where $\beta_{k}$'s are independent Brownian motions.

The interesting thing about this infinite dimensional BM given in \cref{eq:2} is
that while it looks like an element of $L^{2}(T^{d})$, but it turns out not to be. To
see this, observe that for any dimension $d$, we have
\begin{align*}
  \E\left[\norm{B(t)}_{L^{2}(T^{d})}^{2} \right]
  &=\E\left[\sum_{k\in \Z^{d}}\beta_{k}(t)^{2}  \right]
    &&\text{By Parseval}\\
  &\sum_{k\in \Z^{d}}t = \infty.
\end{align*}

But it turns out there is a suitable larger space. Going through the above
calculation but with the $H^{s}$  instead of the $L^{2}$ norm, we see that
\begin{equation*}
  \E\left[\norm{B(t)}_{H^{s}}^{2} \right] <\infty
  \quad \text{ iff} \quad
  s<-d/2
\end{equation*}








\end{document}

\bibliographystyle{siam.bst}
\bibliography{bibliography}
